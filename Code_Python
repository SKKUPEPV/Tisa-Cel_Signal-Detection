pip install pandas
pip intall scikit-learn
pip intall openpyxl
pip install imblearn
pip install matplotlib
pip install xgboost

import pandas as pd
import numpy as np 
import openpyxl as ox
from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler

train=pd.read_excel('C:/Users/Gul2/Desktop/Training_240523.xlsx')
train

train['label'].value_counts()
feature=train.iloc[:,3:32]
label=train['label'] 
feature.head()
#label.head()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(feature, label, test_size=0.20, random_state=92)

from imblearn.over_sampling import SMOTE # Import SMOTE function
sm=SMOTE(random_state=92)

x_resampled, y_resampled = sm.fit_resample(x_train, y_train)

import xgboost as xgb
from xgboost import XGBClassifier 

xgb = xgb.XGBClassifier(randomstate=92)
xgb.fit(x_resampled,y_resampled)
xgb_roc_score = roc_auc_score(y_test, xgb.predict(x_test))
print('GBM AUC: {0:.4f}'.format(xgb_roc_score))

xgb1=XGBClassifier()

xgb_params_grid={
         'n_estimators':[100], 'eta':[0.1],
         'max_depth':[10], 'min_child_weight':[2], 'gamma':[1], 
         'colsample_bytree':[0.5],
         'random_state':[92]
         }
skf_infli=StratifiedKFold(n_splits=5, shuffle=True, random_state=92) #5-fold cross validation
xgb1=GridSearchCV(xgb1, param_grid=xgb_params_grid, scoring="roc_auc", cv=skf_infli)
xgb1.fit(x_resampled,y_resampled)

print("Best performance : {0:.4f}".format(xgb1.best_score_))
print("Best parameters: ", xgb1.best_params_)

def plot_prc_curve_data(recall,precision,label=None):
    plt.plot(recall, precision, marker='.', color='red', label=label)
    plt.axis([0,1,0,1])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.legend(loc="lower right")
    
xgb_roc_score = roc_auc_score(y_test, xgb1.predict_proba(x_test)[:,1], average='micro')

xgb_probs = xgb1.predict_proba(x_test)[:,1]
y_hat = xgb1.predict(x_test)
xgb_precision, xgb_recall, _ = precision_recall_curve(y_test,xgb_probs)
xgb_f1, xgb_auprc, xgb_accuracy = f1_score(y_test,y_hat), auc(xgb_recall,xgb_precision), accuracy_score(y_test,y_hat)

print('검증데이터셋에서 ACCURACY: %.4f' % (xgb_accuracy))
print('검증데이터셋에서 AUROC: %.4f' % (xgb_roc_score)) 
print('검증데이터셋에서 AUPRC: %.4f' % (xgb_auprc))
print('검증데이터셋에서 F1 Score: %.4f' % (xgb_f1))

plot_prc_curve_data(xgb_recall, xgb_precision,label='Tisagenlecleucel(AUPRC:0.9249)')
plt.legend(loc="lower right")
plt.savefig('AUPRC', dpi=300)
plt.show()

def plot_roc_curve(fpr,tpr,label=None):
    plt.plot(fpr, tpr, linewidth=2, color='blue', marker='.', label=label)
    plt.plot([0,1],[0,1],'k--')
    plt.axis([0,1,0,1])
    plt.xlabel('1-Specificity')
    plt.ylabel('Sensitivity')
    plt.legend(loc="lower right")
    
y_probs_xgb = xgb1.predict_proba(x_test)
y_scores_xgb = y_probs_xgb[:,1]
fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, y_scores_xgb)

plot_roc_curve(fpr_xgb, tpr_xgb, label='Tisagenlecleucel(AUROC:0.7577)')
plt.legend(loc="lower right")
plt.savefig('AUROC', dpi=3000)

plt.show()

xgb_auroc = roc_auc_score(y_test, xgb1.predict_proba(x_test)[:,1], average='micro')
print('AUROC: %.4f' % (xgb_auroc))   

y_probs_xgb = xgb1.predict_proba(x_test)
validation = y_probs_xgb[:,1].reshape(-1,1)

import xgboost

fig, ax = plt.subplots(figsize=(15,20))
xgboost.plot_importance(xgb1.best_estimator_, xlabel = ' ', title = 'Feature importance', ax=ax)
plt.savefig('feature importance', dpi=300)

predict=pd.read_excel('C:/Users/Gul2/Desktop/Prediction_240523.xlsx')
predict

preds = xgb1.predict_proba(predict)
preds_df = pd.DataFrame(preds)
preds_df

preds_df.to_excel('C:/Users/Gul2/Desktop/Classification_240524.xlsx')
